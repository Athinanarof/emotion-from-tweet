{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Here we'll train a sentiment analysis model to validate the data from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140_path = Path('../datasets/sentiment140/sentiment140.csv')\n",
    "data = pd.read_csv(sentiment140_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@whiskey_kitten www.Pandora.com - plays music ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>studying for a test I hope not to fail....most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>@BlowhornOz Oh! Doesn't sound so good, I got t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tomorrow is my last day at A&amp;amp;D HS  fml and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Journalism has no future? That sounds pretty m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  @whiskey_kitten www.Pandora.com - plays music ...\n",
       "1      0  studying for a test I hope not to fail....most...\n",
       "2      4  @BlowhornOz Oh! Doesn't sound so good, I got t...\n",
       "3      0  tomorrow is my last day at A&amp;D HS  fml and...\n",
       "4      0  Journalism has no future? That sounds pretty m..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Preprocess the texts:\n",
    "- Convert to Lowercase: Convert all characters from the text to lowercase\n",
    "- Remove special characters: Remove links and usernames and transform emojis to text\n",
    "- Remove repetitions: Remove char repetitions (e.g. whaaaaaat => what)\n",
    "- Remove Stop words: Remove common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rmohashi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from time import time\n",
    "import nltk\n",
    "from emoji import demojize\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up: 78.03 sec\n"
     ]
    }
   ],
   "source": [
    "texts = data.tweet\n",
    "\n",
    "start = time()\n",
    "# Lowercasing\n",
    "texts = texts.str.lower()\n",
    "\n",
    "# Remove special chars\n",
    "texts = texts.str.replace(r\"(http|@)\\S+\", \"\")\n",
    "texts = texts.apply(demojize)\n",
    "texts = texts.str.replace(r\"::\", \": :\")\n",
    "texts = texts.str.replace(r\"â€™\", \"'\")\n",
    "texts = texts.str.replace(r\"[^a-z\\':_]\", \" \")\n",
    "\n",
    "# Remove repetitions\n",
    "pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "texts = texts.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "# Transform short negation form\n",
    "texts = texts.str.replace(r\"(can't|cannot)\", 'can not')\n",
    "texts = texts.str.replace(r\"n't\", ' not')\n",
    "\n",
    "# Remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('not')\n",
    "stopwords.remove('nor')\n",
    "stopwords.remove('no')\n",
    "texts = texts.apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in stopwords])\n",
    ")\n",
    "\n",
    "print(\"Time to clean up: {:.2f} sec\".format(time() - start))\n",
    "\n",
    "data.tweet = texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Transform the text corpus to a vector representation\n",
    "\n",
    "- **num_words**: Number of words to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, lower=True)\n",
    "tokenizer.fit_on_texts(data.tweet)\n",
    "\n",
    "file_to_save = Path('../datasets/sentiment140/tokenizer.pickle').resolve()\n",
    "with file_to_save.open('wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "Split the dataset in train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(columns=['label', 'tweet'])\n",
    "validation = pd.DataFrame(columns=['label', 'tweet'])\n",
    "for label in data.label.unique():\n",
    "    label_data = data[data.label == label]\n",
    "    train_data, validation_data = train_test_split(label_data, test_size=0.3)\n",
    "    train = pd.concat([train, train_data])\n",
    "    validation = pd.concat([validation, validation_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Define the Bidirectional GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, GRU\n",
    "from tensorflow.keras.layers import Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = min(tokenizer.num_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 200\n",
    "input_length = 100\n",
    "gru_units = 128\n",
    "gru_dropout = 0.1\n",
    "recurrent_dropout = 0.1\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 13:36:20.397812 140315330369344 deprecation.py:506] From /home/rmohashi/anaconda3/envs/emodata/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0716 13:36:20.410246 140315330369344 deprecation.py:506] From /home/rmohashi/anaconda3/envs/emodata/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0716 13:36:20.413324 140315330369344 deprecation.py:506] From /home/rmohashi/anaconda3/envs/emodata/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0716 13:36:20.413828 140315330369344 deprecation.py:506] From /home/rmohashi/anaconda3/envs/emodata/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0716 13:36:20.414215 140315330369344 deprecation.py:506] From /home/rmohashi/anaconda3/envs/emodata/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=embedding_dim,\n",
    "    input_shape=(input_length,)\n",
    "))\n",
    "\n",
    "model.add(Bidirectional(GRU(\n",
    "    gru_units,\n",
    "    return_sequences=True,\n",
    "    dropout=gru_dropout,\n",
    "    recurrent_dropout=recurrent_dropout\n",
    ")))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0716 13:36:20.902724 140315330369344 deprecation.py:323] From /home/rmohashi/anaconda3/envs/emodata/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 256)          252672    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,260,929\n",
      "Trainable params: 2,260,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "Prepare the model input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [text.split() for text in train.tweet]\n",
    "validation_sequences = [text.split() for text in validation.tweet]\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_sequences)\n",
    "list_tokenized_validation = tokenizer.texts_to_sequences(validation_sequences)\n",
    "\n",
    "x_train = pad_sequences(list_tokenized_train, maxlen=input_length)\n",
    "x_validation = pad_sequences(list_tokenized_validation, maxlen=input_length)\n",
    "y_train = train.label.replace(4, 1)\n",
    "y_validation = validation.label.replace(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Do the training process with the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280000 samples, validate on 120000 samples\n",
      "280000/280000 [==============================] - 374s 1ms/sample - loss: 0.4637 - acc: 0.7804 - val_loss: 0.4366 - val_acc: 0.7937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d0987c940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_validation, y_validation),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = Path('../models/sentiment_analysis/gru_model.h5').resolve()\n",
    "model.save_weights(model_file.as_posix())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
